# =============================
# Install dependencies if needed
# =============================
# !pip install sacrebleu bert-score transformers datasets matplotlib

import re
import pandas as pd
import matplotlib.pyplot as plt
from datasets import Dataset, DatasetDict
from transformers import (
    MarianTokenizer,
    MarianMTModel,
    DataCollatorForSeq2Seq,
    Seq2SeqTrainer,
    Seq2SeqTrainingArguments,
    EarlyStoppingCallback
)
from sacrebleu import corpus_bleu, corpus_chrf
from bert_score import score as bert_score

# =============================
# Arabic Normalization
# =============================
def normalize_arabic(text):
    text = re.sub(r'[\u0617-\u061A\u064B-\u0652]', '', text)  # remove diacritics
    text = re.sub(r'ـ', '', text)                             # remove tatweel
    text = re.sub(r'[إأٱآا]', 'ا', text)                      # normalize Alef
    text = re.sub(r'ى', 'ي', text)                            # normalize Ya
    text = re.sub(r'ة', 'ه', text)                            # Ta marbuta → Ha
    text = re.sub(r'\s+', ' ', text).strip()
    return text

# =============================
# Load Data
# =============================
df = pd.read_csv("data_preprocessed.csv")
df = df.dropna(subset=["en_clean", "ar_clean"])
df = df.sample(frac=1).reset_index(drop=True)

split = int(0.9 * len(df))
train_df = df[:split]
test_df = df[split:]

dataset = DatasetDict({
    "train": Dataset.from_pandas(train_df),
    "test": Dataset.from_pandas(test_df),
})

print("Train size:", len(dataset["train"]))
print("Test size:", len(dataset["test"]))

# =============================
# Load Model & Tokenizer
# =============================
model_name = "marefa-nlp/marefa-mt-en-ar"
tokenizer = MarianTokenizer.from_pretrained(model_name)
model = MarianMTModel.from_pretrained(model_name)

# =============================
# Preprocess Data
# =============================
max_input_length = 128
max_target_length = 128

def preprocess_function(examples):
    inputs = tokenizer(examples["en_clean"], max_length=max_input_length, truncation=True, padding="max_length")
    targets = tokenizer(examples["ar_clean"], max_length=max_target_length, truncation=True, padding="max_length")
    inputs["labels"] = targets["input_ids"]
    return inputs

tokenized = dataset.map(preprocess_function, batched=True, remove_columns=dataset["train"].column_names)

# =============================
# Data Collator
# =============================
data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)

# =============================
# Training Arguments
# =============================
training_args = Seq2SeqTrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    gradient_accumulation_steps=2,
    weight_decay=0.01,
    num_train_epochs=30,
    predict_with_generate=True,
    logging_dir="./logs",
    logging_steps=50,
    save_total_limit=3,
    load_best_model_at_end=True,
    metric_for_best_model="bleu",
    label_smoothing_factor=0.2,
    lr_scheduler_type="linear",
    warmup_steps=500,
    fp16=True,  # requires GPU
)

# =============================
# Metrics: BLEU + chrF + BERTScore (AraBERT)
# =============================
arabert_model = "aubmindlab/bert-base-arabertv2"

def compute_metrics(eval_preds):
    preds, labels = eval_preds
    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    # Normalize Arabic
    decoded_preds = [normalize_arabic(p) for p in decoded_preds]
    decoded_labels = [normalize_arabic(l) for l in decoded_labels]
    decoded_labels = [[lbl] for lbl in decoded_labels]

    # BLEU
    bleu = corpus_bleu(decoded_preds, decoded_labels).score

    # chrF
    chrf = corpus_chrf(decoded_preds, decoded_labels).score

    # BERTScore with AraBERT
    P, R, F1 = bert_score(
        decoded_preds,
        [l[0] for l in decoded_labels],
        model_type=arabert_model,
        lang="ar"
    )

    return {
        "bleu": bleu,
        "chrf": chrf,
        "bert_f1": F1.mean().item()
    }

# =============================
# Trainer
# =============================
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized["train"],
    eval_dataset=tokenized["test"],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],
)

# =============================
# Train
# =============================
trainer.train()

# =============================
# Save Final Model
# =============================
trainer.save_model("./finetuned-en-ar")

# =============================
# Plot Training Curves
# =============================
metrics = trainer.state.log_history
df_metrics = pd.DataFrame(metrics)
df_metrics.to_csv("training_metrics.csv", index=False)

eval_df = df_metrics.dropna(subset=["eval_loss"])

plt.figure(figsize=(12,6))

# Loss
plt.subplot(1, 2, 1)
plt.plot(eval_df["epoch"], eval_df["loss"], label="Train Loss")
plt.plot(eval_df["epoch"], eval_df["eval_loss"], label="Val Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.title("Training vs Validation Loss")
plt.legend()

# BLEU/chrF/BERT F1
plt.subplot(1, 2, 2)
plt.plot(eval_df["epoch"], eval_df["eval_bleu"], label="BLEU")
plt.plot(eval_df["epoch"], eval_df["eval_chrf"], label="chrF")
plt.plot(eval_df["epoch"], eval_df["eval_bert_f1"], label="BERT F1")
plt.xlabel("Epoch")
plt.ylabel("Score")
plt.title("Evaluation Metrics")
plt.legend()

plt.tight_layout()
plt.show()

# =============================
# Test Translation
# =============================
text = "Hello, how are you?"
inputs = tokenizer([text], return_tensors="pt", padding=True).to(model.device)

outputs = model.generate(
    **inputs,
    max_length=128,
    num_beams=10,
    length_penalty=1.2,
    early_stopping=True
)

print("English:", text)
print("Arabic:", tokenizer.decode(outputs[0], skip_special_tokens=True))
