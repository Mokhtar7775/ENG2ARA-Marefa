
import re
import pandas as pd
import matplotlib.pyplot as plt
from datasets import Dataset, DatasetDict
from transformers import (
    MarianTokenizer,
    MarianMTModel,
    DataCollatorForSeq2Seq,
    Seq2SeqTrainer,
    Seq2SeqTrainingArguments,
)
from sacrebleu import corpus_bleu, corpus_chrf
from bert_score import score as bert_score

# =============================
# Arabic Normalization
# =============================
def normalize_arabic(text):
    if pd.isna(text):
        return ""
    text = re.sub(r'[\u0617-\u061A\u064B-\u0652]', '', text)  
    text = re.sub(r'ـ', '', text)                            
    text = re.sub(r'[إأٱآا]', 'ا', text)                      
    text = re.sub(r'ى', 'ي', text)                           
    text = re.sub(r'ة', 'ه', text)                            
    text = re.sub(r'\s+', ' ', text).strip()
    return text

# =============================
# Load Data
# =============================
df = pd.read_csv("/content/drive/MyDrive/data_cleaned.csv")
df = df.dropna(subset=["en_clean", "ar_clean"])
df["ar_clean"] = df["ar_clean"].apply(normalize_arabic)

split = int(0.9 * len(df))
train_df, test_df = df[:split], df[split:]

dataset = DatasetDict({
    "train": Dataset.from_pandas(train_df),
    "test": Dataset.from_pandas(test_df),
})
print("Train size:", len(dataset["train"]))
print("Test size:", len(dataset["test"]))

# =============================
# Load Model & Tokenizer
# =============================
model_name = "marefa-nlp/marefa-mt-en-ar"
tokenizer = MarianTokenizer.from_pretrained(model_name)
model = MarianMTModel.from_pretrained(model_name)

# =============================
# Preprocess Data
# =============================
max_input_length, max_target_length = 128, 128

def preprocess_function(examples):
    inputs = tokenizer(examples["en_clean"], max_length=max_input_length, truncation=True, padding="max_length")
    targets = tokenizer([normalize_arabic(t) for t in examples["ar_clean"]],
                        max_length=max_target_length, truncation=True, padding="max_length")
    inputs["labels"] = targets["input_ids"]
    return inputs

tokenized = dataset.map(preprocess_function, batched=True, remove_columns=dataset["train"].column_names)

# =============================
# Data Collator
# =============================
data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)

# =============================
# Custom Trainer with anti-repetition decoding
# =============================
class CustomSeq2SeqTrainer(Seq2SeqTrainer):
    def prediction_step(self, model, inputs, prediction_loss_only, ignore_keys=None):
        generation_kwargs = {
            "max_length": 128,
            "num_beams": 5,
            "no_repeat_ngram_size": 3,
            "repetition_penalty": 2.0,
            "length_penalty": 1.2,
            "early_stopping": True,
        }
        return super().prediction_step(
            model,
            inputs,
            prediction_loss_only,
            ignore_keys=ignore_keys,
            **generation_kwargs
        )

# =============================
# Training Arguments
# =============================
training_args = Seq2SeqTrainingArguments(
    output_dir="/content/drive/MyDrive/NewResults",
    evaluation_strategy="epoch",    
    save_strategy="no",              
    learning_rate=3e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    gradient_accumulation_steps=2,
    weight_decay=0.01,
    num_train_epochs=15,            
    predict_with_generate=True,
    generation_max_length=128,
    generation_num_beams=5,
    logging_dir="/content/drive/MyDrive/logs",
    logging_steps=50,
    load_best_model_at_end=False,   
    label_smoothing_factor=0.1,
    lr_scheduler_type="linear",
    warmup_steps=500,
    fp16=True,
)

# =============================
# Metrics
# =============================
def compute_metrics(eval_preds):
    preds, labels = eval_preds
    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    decoded_preds = [normalize_arabic(p) for p in decoded_preds]
    decoded_labels = [normalize_arabic(l) for l in decoded_labels]
    decoded_labels = [[lbl] for lbl in decoded_labels]

    bleu = corpus_bleu(decoded_preds, decoded_labels).score
    chrf = corpus_chrf(decoded_preds, decoded_labels).score
    P, R, F1 = bert_score(decoded_preds, [l[0] for l in decoded_labels], model_type="bert-base-multilingual-cased", lang="ar")

    return {"bleu": bleu, "chrf": chrf, "bert_f1": F1.mean().item()}

# =============================
# Trainer
# =============================
trainer = CustomSeq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized["train"],
    eval_dataset=tokenized["test"],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

# =============================
# Train
# =============================
trainer.train()

# =============================
# Save Final Model
# =============================
trainer.save_model("/content/drive/MyDrive/finetuned-en-ar")

# =============================
# Test Translation
# =============================
text = "Hello, how are you?"
inputs = tokenizer([text], return_tensors="pt", padding=True).to(model.device)
outputs = model.generate(
    **inputs,
    max_length=128,
    num_beams=5,
    no_repeat_ngram_size=3,
    repetition_penalty=2.0,
    length_penalty=1.2,
    early_stopping=True
)
print("EN:", text)
print("AR:", tokenizer.decode(outputs[0], skip_special_tokens=True))
